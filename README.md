# LP34_web_AAM
Пользователь регистрируется на сайте.
После этого пользователь заполняет форму на странице расчётов, перейдя по кнопке меню "Ввод данных".
форма заполняется несколько раз, в зависимости от задачи, и при каждом нажатии кнопки "Добавить" в базе данных формируется строка с введёнными данными, временем формирования строки и user_id.
После завершения ввода всех данных нажимаем кнопку "Сохранить DataSet", при этом формируется файл с расширением .csv для дальнейшей обработки и запуска расчёта по классификации по кнопке "Расчитать" 

*Пример формирования таблицы:*

| Наименование показателя |  Максимальное количество баллов| Набранные баллы | Претендент|
|---------|-----------|-----------|-------------|
| Тестовое задание 1 | 10 | 9 | 1 |
| Тестовое задание 1 | 10 | 7 |  2  |
| Тестовое задание 2 | 5 | 4 | 1  |
| Тестовое задание 2 | 5 | 5 | 2 |
| Скорость выполнения задания | 10 | 7 | 1  |
| Скорость выполнения задания | 10 | 5 | 2 |
| Ответ на вопрос 1| 5 | 4 | 1  |
| Ответ на вопрос 1| 5 | 4 | 2 |
| Ответ на вопрос 2| 5 | 3 | 1  |
| Ответ на вопрос 2| 5 | 5 | 2 |
| Образование | 10 | 7 | 1  |
| Образование | 10 | 8 | 2 |
| Запрашиваемая зарплата | 10 | 12 | 1  |
| Запрашиваемая зарплата | 10 | 9 | 2 |


## **Наивный байесовский классификатор**
Наивный байесовский классификатор (Naive Bayes classifier) — вероятностный классификатор на основе формулы Байеса со строгим (наивным) предположением о независимости признаков между собой при заданном классе, что сильно упрощает задачу классификации из-за оценки одномерных вероятностных плотностей вместо одной многомерной.

В данном случае, одномерная вероятностная плотность — это оценка вероятности каждого признака отдельно при условии их независимости, а многомерная — оценка вероятности комбинации всех признаков, что вытекает из случая их зависимости. Именно по этой причине данный классификатор называется наивным, поскольку позволяет сильно упростить вычисления и повысить эффективность алгоритма. Однако такое предположение не всегда является верным на практике и в ряде случаев может привести к значительному ухудшению качества прогнозов.

Сама же формула Байеса выглядит следующим образом:

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

где:
- $P(A|B)$ — апостериорная вероятность события A при условии выполнения события B;

- $P(B|A)$ — условная вероятность события B при условии выполнения события A;

- $P(A)$ и $P(B)$ — априорные вероятности событий A и B соответственно.

А в контексте машинного обучения формула Байеса приобретает следующий вид:

$$P(y_k|X) = \frac{P(y_k)P(X|y_k)}{P(X)}$$

где:
- $P(y_k|X)$ — апостериорная вероятность принадлежности образца к классу $y_k$ с учётом его признаков $X$;
- $P(X|y_k)$ — правдоподобие, то есть вероятность признаков $X$ при заданном классе $y_k$;
- $P(y_k)$ — априорная вероятность принадлежности случайно выбранного наблюдения к классу $y_k$;
- $P(X)$ — априорная вероятность признаков $X$.

Если объект описывается не одним, а несколькими признаками $X_1, X_2, ..., X_n$, то формула принимает вид:

$$P(y_k|X_1, X_2, ..., X_n) = \frac{P(y_k)\prod_{i=1}^n P(X_i|y_k)}{P(X_1, X_2, ..., X_n)}$$

На практике числитель данной формулы представляет наибольший интерес, поскольку знаменатель зависит только от признаков, а не от класса, и поэтому часто он опускается при сравнении вероятностей разных классов. В конечном счёте правило классификации будет пропорционально выбору класса с максимальной апостериорной вероятностью:

$$y_k \propto \arg\max_{y_k} P(y_k)\prod_{i=1}^n P(X_i|y_k)$$

Для оценки параметров модели, то есть вероятностей $P(y_k)$ и $P(X_i|y_k)$, обычно применяется метод максимального правдоподобия, который в данном случае основан на частотах встречаемости классов и признаков в обучающей выборке.

### **Разновидности наивного Байеса**
В библиотеке scikit-learn есть несколько реализаций наивного байесовского классификатора, отличающиеся предположениями о распределении признаков при заданном классе. К таковым относятся следующие:

- **Гауссовский наивный байесовский классификатор (GaussianNB)** — вариант для работы с непрерывными признаками, которые имеют нормальное (гауссовское) распределение. Вероятность признака при заданном классе вычисляется по формуле: $$P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma_y^2}}\exp\left(-\frac{(x_i-\mu_y)^2}{2\sigma_y^2}\right)$$ где $\mu_y$ и $\sigma_y$ — это среднее и стандартное отклонения признака в классе $y$. Эти параметры оцениваются с помощью метода максимального правдоподобия по обучающим данным.
- **Мультиномиальный наивный байесовский классификатор (MultinomialNB)** — вариант для работы с дискретными признаками, которые имеют мультиномиальное распределение. Такие признаки часто встречаются в задачах классификации текстов, где они представляют собой количество вхождений в тексте. Вероятность признака при заданном классе вычисляется по формуле: $$P(x_i|y) = \frac{N_{yi} + \alpha}{N_y + \alpha n}$$ где $N_{yi}$ — это количество раз, когда признак $i$ встречается в классе $y$; $N_y$ — общее количество всех признаков в классе $y$; $n$ — количество различных признаков; а $\alpha$ — сглаживающий параметр, предотвращающий возникновение нулевых вероятностей.
- **Комплементарный наивный байесовский классификатор (ComplementNB)** — улучшенный вариант *MultinomialNB*, подходящий для несбалансированных наборов данных. Вместо оценки вероятности признака при заданном классе, алгоритм оценивает нормированный вес признака $w_{ci}$ для класса $c$ как вероятность признака при дополнении класса, то есть при всех остальных классах. Таким образом, алгоритм учитывает не только частоту признаков в классе, но и их отсутствие в других классах, что делает его менее чувствительным к смещению выборки. Формула для вычисления вероятности признака при дополнении класса выглядит следующим образом: \begin{align}\begin{aligned}\hat{\theta}_{ci} = \frac{\alpha_i + \sum_{j:y_j \neq c} d_{ij}}
                         {\alpha + \sum_{j:y_j \neq c} \sum_{k} d_{kj}}\\w_{ci} = \log \hat{\theta}_{ci}\\w_{ci} = \frac{w_{ci}}{\sum_{j} |w_{cj}|}\end{aligned}\end{align}
где $\hat{\theta}_{ci}$ — это оценка вероятности признака $i$ при дополнении класса $c$, которая вычисляется с помощью сглаживающего параметра $\alpha_i$ и частоты признака $i$ во всех классах кроме $c$ (в данном случае $d_{ij}$ — это количество раз, когда признак $i$ встречается в классе $j$); $w_{ci}$ — это нормированный вес признака $i$ для класса $c$. Предсказанный класс $\hat c$ для заданного вектора признаков $t$ будет выглядеть следующим образом: $$\hat{c} = \arg\min_c \sum_{i} t_i w_{ci}$$
- **Бернуллиевский наивный байесовский классификатор (BernoulliNB)** — ещё один вариант для работы с дискретными признаками, но которые имеют бернуллиевское распределение. В данном случае признаки представляют собой бинарные индикаторы наличия или отсутствия определённых свойств в объекте. Например, в задаче классификации текстов это может быть наличие или отсутствие определённых слов в тексте. Вероятность признака при заданном классе вычисляется по формуле: $$P(x_i|y) = P(x_i = 1|y)x_i + (1-P(x_i = 1|y))(1-x_i)$$ где $P(x_i = 1|y)$ — это вероятность того, что признак $i$ принимает значение 1 (истина) при условии, что объект принадлежит классу $y$; $x_i$ — значение признака $i$ (0 или 1).
- **Категориальный наивный байесовский классификатор (CategoricalNB)** — вариант для категориально распределенных данных, основанный на предположении, что каждый описываемый индексом признак имеет своё собственное категориальное распределение. Вероятность признака при заданном классе вычисляется по формуле: $$P(x_i = t \mid y = c \: ;\, \alpha) = \frac{ N_{tic} + \alpha}{N_{c} + \alpha n_i}$$ где $N_{tic} = |\{j \in J \mid x_{ij} = t, y_j = c\}|$ — это количество раз, когда признак $x_i$ принимает значение $t$ в классе $c$; $N_{c} = |\{ j \in J\mid y_j = c\}|$ — общее количество всех признаков в классе $c$ в обучающих данных; $\alpha$ — сглаживающий параметр; $n_i$ — количество доступных значений признака $i$.

### **Принцип работы наивного байесовского классификатора c гауссовским распределением**
Алгоритм строится следующим образом:
- 1) изначально рассчитываются априорные вероятности классов;
- 2) после рассчитываются средние и стандартные отклонения признаков по классам;
- 3) на основе полученных отклонений признаков по классам рассчитывается вероятностная плотность тестовых признаков по распределению Гаусса;
- 4) далее вычисляются апостериорные вероятности как произведение априорных вероятностей классов и вероятностных плотностей тестовых признаков;
- 5) классы с максимальной апостериорной вероятностью будут итоговым прогнозом.

| Наименование показателя |  Максимальное количество баллов| Набранные баллы | Претендент|
|---------|-----------|-----------|-------------|
| Тестовое задание 1 | 10 | 9 | 1 |
| Тестовое задание 1 | 10 | 7 |  2  |
| Тестовое задание 2 | 5 | 4 | 1  |
| Тестовое задание 2 | 5 | 5 | 2 |
| Скорость выполнения задания | 10 | 7 | 1  |
| Скорость выполнения задания | 10 | 5 | 2 |
| Ответ на вопрос 1| 5 | 4 | 1  |
| Ответ на вопрос 1| 5 | 4 | 2 |
| Ответ на вопрос 2| 5 | 3 | 1  |
| Ответ на вопрос 2| 5 | 5 | 2 |
| Образование | 10 | 7 | 1  |
| Образование | 10 | 8 | 2 |
| Запрашиваемая зарплата | 10 | 12 | 1  |
| Запрашиваемая зарплата | 10 | 9 | 2 |
